"""
üß† LLM Client - –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback
–ü–æ–¥–¥–µ—Ä–∂–∫–∞: OpenAI, DeepSeek, xAI Grok, Google Gemini, Cohere, HuggingFace
"""

import requests
from typing import Optional, List, Dict, Any
from config import Config
import logging

logger = logging.getLogger(__name__)


class LLMProvider:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
    
    def __init__(self, name: str, api_key: str, model: str):
        self.name = name
        self.api_key = api_key
        self.model = model
        self.is_available = bool(api_key)
    
    def generate(self, prompt: str, system_prompt: str = "", temperature: float = 0.7, max_tokens: int = 1000) -> Optional[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤ –Ω–∞—Å–ª–µ–¥–Ω–∏–∫–∞—Ö)"""
        raise NotImplementedError


class OpenAIProvider(LLMProvider):
    """OpenAI / DeepSeek / xAI (OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ API)"""
    
    def __init__(self, name: str, api_key: str, model: str, base_url: Optional[str] = None):
        super().__init__(name, api_key, model)
        self.base_url = base_url
        if self.is_available:
            try:
                from openai import OpenAI
                self.client = OpenAI(
                    api_key=api_key,
                    base_url=base_url
                ) if base_url else OpenAI(api_key=api_key)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è {name}: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∏–µ–Ω—Ç: {e}")
                self.is_available = False
    
    def generate(self, prompt: str, system_prompt: str = "", temperature: float = 0.7, max_tokens: int = 1000) -> Optional[str]:
        try:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è {self.name}: –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return None


class GoogleGeminiProvider(LLMProvider):
    """Google AI Studio (Gemini)"""
    
    def __init__(self, name: str, api_key: str, model: str = "gemini-1.5-flash"):
        super().__init__(name, api_key, model)
        if self.is_available:
            try:
                import google.generativeai as genai
                genai.configure(api_key=api_key)
                self.client = genai.GenerativeModel(model)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è {name}: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∏–µ–Ω—Ç: {e}")
                self.is_available = False
    
    def generate(self, prompt: str, system_prompt: str = "", temperature: float = 0.7, max_tokens: int = 1000) -> Optional[str]:
        try:
            # Gemini –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–π system prompt, –∫–æ–º–±–∏–Ω–∏—Ä—É–µ–º
            full_prompt = f"{system_prompt}\n\n{prompt}" if system_prompt else prompt
            
            generation_config = {
                "temperature": temperature,
                "max_output_tokens": max_tokens,
            }
            
            response = self.client.generate_content(
                full_prompt,
                generation_config=generation_config
            )
            return response.text.strip()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è {self.name}: –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return None


class CohereProvider(LLMProvider):
    """Cohere AI"""
    
    def __init__(self, name: str, api_key: str, model: str = "command-r-plus"):
        super().__init__(name, api_key, model)
        if self.is_available:
            try:
                import cohere
                self.client = cohere.Client(api_key)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è {name}: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∏–µ–Ω—Ç: {e}")
                self.is_available = False
    
    def generate(self, prompt: str, system_prompt: str = "", temperature: float = 0.7, max_tokens: int = 1000) -> Optional[str]:
        try:
            # Cohere –∏—Å–ø–æ–ª—å–∑—É–µ—Ç preamble –¥–ª—è system prompt
            preamble = system_prompt if system_prompt else None
            
            response = self.client.chat(
                message=prompt,
                preamble=preamble,
                model=self.model,
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response.text.strip()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è {self.name}: –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return None


class HuggingFaceProvider(LLMProvider):
    """HuggingFace Inference API"""
    
    def __init__(self, name: str, api_key: str, model: str = "mistralai/Mixtral-8x7B-Instruct-v0.1"):
        super().__init__(name, api_key, model)
        self.api_url = f"https://api-inference.huggingface.co/models/{model}"
        self.headers = {"Authorization": f"Bearer {api_key}"}
    
    def generate(self, prompt: str, system_prompt: str = "", temperature: float = 0.7, max_tokens: int = 1000) -> Optional[str]:
        try:
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            full_prompt = f"{system_prompt}\n\n{prompt}" if system_prompt else prompt
            
            payload = {
                "inputs": full_prompt,
                "parameters": {
                    "temperature": temperature,
                    "max_new_tokens": max_tokens,
                    "return_full_text": False
                }
            }
            
            response = requests.post(
                self.api_url,
                headers=self.headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                if isinstance(result, list) and len(result) > 0:
                    return result[0].get("generated_text", "").strip()
            return None
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è {self.name}: –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return None


class LLMClient:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback –º–µ–∂–¥—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏"""
    
    def __init__(self):
        self.temperature = Config.LLM_TEMPERATURE
        self.providers: List[LLMProvider] = []
        self.current_provider_index = 0
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
        self._initialize_providers()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω—ã–µ
        self.providers = [p for p in self.providers if p.is_available]
        
        if not self.providers:
            logger.error("‚ùå –ù–∏ –æ–¥–∏–Ω LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ API –∫–ª—é—á–∏ –≤ .env")
            raise ValueError("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤")
        
        logger.info(f"‚úÖ –î–æ—Å—Ç—É–ø–Ω—ã–µ LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã: {[p.name for p in self.providers]}")
    
    def _initialize_providers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
        
        # –¢–æ–ª—å–∫–æ –ü–†–û–í–ï–†–ï–ù–ù–´–ï —Ä–∞–±–æ—á–∏–µ –º–æ–¥–µ–ª–∏
        default_models = {
            "deepseek": "deepseek-chat",
        }
        
        use_custom_model = Config.LLM_MODEL != 'auto' and Config.LLM_PROVIDER != 'auto'
        
        # DeepSeek - –†–ê–ë–û–¢–ê–ï–¢, –µ—Å–ª–∏ –µ—Å—Ç—å –±–∞–ª–∞–Ω—Å
        if Config.DEEPSEEK_API_KEY:
            model = Config.LLM_MODEL if (use_custom_model and Config.LLM_PROVIDER == 'deepseek') else default_models["deepseek"]
            self.providers.append(OpenAIProvider("DeepSeek", Config.DEEPSEEK_API_KEY, model, "https://api.deepseek.com"))
    
    def _generate_with_fallback(self, prompt: str, system_prompt: str = "", temperature: float = None, max_tokens: int = 1000) -> Optional[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º –º–µ–∂–¥—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏"""
        if temperature is None:
            temperature = self.temperature
        
        # –ü—Ä–æ–±—É–µ–º –≤—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –ø–æ –æ—á–µ—Ä–µ–¥–∏
        for i, provider in enumerate(self.providers):
            logger.info(f"ü§ñ –ü–æ–ø—ã—Ç–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ {provider.name} (–º–æ–¥–µ–ª—å: {provider.model})...")
            result = provider.generate(prompt, system_prompt, temperature, max_tokens)
            
            if result:
                logger.info(f"‚úÖ {provider.name}: –£—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Ç–µ–∫—Å—Ç")
                # –ü–µ—Ä–µ—Å—Ç–∞–≤–ª—è–µ–º —É—Å–ø–µ—à–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–∞ –ø–µ—Ä–≤–æ–µ –º–µ—Å—Ç–æ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ä–∞–∑–∞
                if i > 0:
                    self.providers[0], self.providers[i] = self.providers[i], self.providers[0]
                return result
            else:
                logger.warning(f"‚ö†Ô∏è {provider.name}: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π...")
        
        logger.error("‚ùå –í—Å–µ LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã!")
        return None
    
    def rewrite_text(self, original_text: str, has_links: bool = True) -> str:
        """
        –ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞, –¥–µ–ª–∞—è –µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º
        
        Args:
            original_text: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞
            has_links: –ï—Å—Ç—å –ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ —Å—Å—ã–ª–∫–∏ –¥–ª—è –∑–∞–º–µ–Ω—ã
            
        Returns:
            –ü–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        try:
            if not original_text or len(original_text.strip()) < 10:
                return original_text
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞–ª–∏—á–∏—è —Å—Å—ã–ª–æ–∫
            if has_links:
                prompt = self._build_rewrite_prompt_with_links(original_text)
            else:
                prompt = self._build_rewrite_prompt_simple(original_text)
            
            system_prompt = "–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π SMM-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π —É–º–µ–µ—Ç –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å –ø–æ—Å—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è —Å–º—ã—Å–ª, –Ω–æ –¥–µ–ª–∞—è –∏—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –∏ –∞–≤—Ç–æ—Ä—Å–∫–∏–º–∏."
            
            # –í—ã–∑–æ–≤ —Å fallback
            result = self._generate_with_fallback(prompt, system_prompt)
            
            if result:
                return result
            else:
                # –ï—Å–ª–∏ –≤—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª —Å –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–µ–π
                logger.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback: –ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞")
                return self._simple_text_modification(original_text)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            return original_text
    
    def _simple_text_modification(self, text: str) -> str:
        """–ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –µ—Å–ª–∏ –≤—Å–µ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã"""
        # –î–æ–±–∞–≤–ª—è–µ–º –±—Ä–µ–Ω–¥–∏–Ω–≥ –∏ –ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é
        modified = f"{text}\n\nüîí –ó–∞—â–∏—Ç–∏—Å—å –æ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ —Å {Config.YOUR_BRAND_NAME}: {Config.YOUR_LINK}"
        return modified
    
    def _build_rewrite_prompt_with_links(self, text: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å—Å—ã–ª–∫–∞–º–∏"""
        return f"""–ü–µ—Ä–µ–ø–∏—à–∏ —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç –æ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞—Ö –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ —Ç–∞–∫, —á—Ç–æ–±—ã –æ–Ω –±—ã–ª —É–Ω–∏–∫–∞–ª—å–Ω—ã–º, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–ª –≤–µ—Å—å —Å–º—ã—Å–ª:

–û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–ö–°–¢:
{text}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
1. –ó–∞–º–µ–Ω–∏ –í–°–ï —Å—Å—ã–ª–∫–∏ (t.me/..., https://...) –Ω–∞ "{Config.YOUR_LINK}"
2. –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç, —Å–æ—Ö—Ä–∞–Ω—è—è –≤—Å–µ —Ñ–∞–∫—Ç—ã –∏ –¥–µ—Ç–∞–ª–∏
3. –î–æ–±–∞–≤—å –∫–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é –ø—Ä–æ VPN –≤ –∫–æ–Ω—Ü–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
4. –ò—Å–ø–æ–ª—å–∑—É–π —Å—Ç–∏–ª—å: {Config.CHANNEL_STYLE}
5. –£–ø–æ–º—è–Ω–∏ "{Config.YOUR_BRAND_NAME}" –∫–∞–∫ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
6. –°–æ—Ö—Ä–∞–Ω–∏ –≤—Å–µ –≤–∞–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: —Ä–µ–≥–∏–æ–Ω—ã, –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã, –≤—Ä–µ–º—è
7. –°–¥–µ–ª–∞–π —Ç–µ–∫—Å—Ç –∂–∏–≤—ã–º –∏ —Å—Ä–æ—á–Ω—ã–º, –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º
8. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π —Ñ—Ä–∞–∑—ã —Ç–∏–ø–∞ "–ø–æ –¥–∞–Ω–Ω—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞" - –ø–∏—à–∏ –æ—Ç —Å–≤–æ–µ–≥–æ –ª–∏—Ü–∞

–ü–ï–†–ï–ü–ò–°–ê–ù–ù–´–ô –¢–ï–ö–°–¢:"""
    
    def _build_rewrite_prompt_simple(self, text: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ —Å—Å—ã–ª–æ–∫"""
        return f"""–°–ª–µ–≥–∫–∞ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç –æ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞—Ö –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –≤—Å–µ —Ñ–∞–∫—Ç—ã:

–û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–ö–°–¢:
{text}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
1. –°–æ—Ö—Ä–∞–Ω–∏ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ: —Ä–µ–≥–∏–æ–Ω—ã, –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã, –≤—Ä–µ–º—è
2. –°–ª–µ–≥–∫–∞ –∏–∑–º–µ–Ω–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
3. –î–æ–±–∞–≤—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ "{Config.YOUR_BRAND_NAME}" –∫–∞–∫ —Ä–µ—à–µ–Ω–∏—è (–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ)
4. –ò—Å–ø–æ–ª—å–∑—É–π —Å—Ç–∏–ª—å: {Config.CHANNEL_STYLE}
5. –°–¥–µ–ª–∞–π —Ç–µ–∫—Å—Ç –Ω–µ–º–Ω–æ–≥–æ –±–æ–ª–µ–µ –∂–∏–≤—ã–º

–ü–ï–†–ï–ü–ò–°–ê–ù–ù–´–ô –¢–ï–ö–°–¢:"""
    
    def check_uniqueness(self, original: str, rewritten: str) -> float:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ)
        
        Returns:
            –ü—Ä–æ—Ü–µ–Ω—Ç —Ä–∞–∑–ª–∏—á–∏—è (0-100)
        """
        # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–ª–∏—á–∞—é—â–∏—Ö—Å—è —Å–ª–æ–≤
        original_words = set(original.lower().split())
        rewritten_words = set(rewritten.lower().split())
        
        if not original_words:
            return 0.0
        
        different_words = rewritten_words - original_words
        uniqueness = (len(different_words) / len(original_words)) * 100
        
        return min(uniqueness, 100.0)
    
    def enhance_with_cta(self, text: str) -> str:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç Call-to-Action –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        
        Args:
            text: –¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞
            
        Returns:
            –¢–µ–∫—Å—Ç —Å CTA
        """
        try:
            prompt = f"""–î–æ–±–∞–≤—å –∫–æ—Ä–æ—Ç–∫–∏–π Call-to-Action (–ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é) –≤ –∫–æ–Ω–µ—Ü —ç—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:

–¢–ï–ö–°–¢:
{text}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
1. CTA –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—Ä–æ –∑–∞—â–∏—Ç—É –æ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ —á–µ—Ä–µ–∑ VPN
2. –£–ø–æ–º—è–Ω–∏ "{Config.YOUR_BRAND_NAME}"
3. –î–æ–±–∞–≤—å —Å—Å—ã–ª–∫—É "{Config.YOUR_LINK}"
4. –ú–∞–∫—Å–∏–º—É–º 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
5. –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –≤ —Ç–µ–∫—Å—Ç

–¢–ï–ö–°–¢ –° CTA:"""
            
            result = self._generate_with_fallback(prompt, "", 0.5, 500)
            
            if result:
                return result
            else:
                # –ü—Ä–æ—Å—Ç–æ–π CTA –≤—Ä—É—á–Ω—É—é
                return f"{text}\n\nüîí –ó–∞—â–∏—Ç–∏—Ç–µ—Å—å –æ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ —Å {Config.YOUR_BRAND_NAME}: {Config.YOUR_LINK}"
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ CTA: {e}")
            return f"{text}\n\nüîí –ó–∞—â–∏—Ç–∏—Ç–µ—Å—å –æ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ —Å {Config.YOUR_BRAND_NAME}: {Config.YOUR_LINK}"


# Singleton instance
_llm_client = None

def get_llm_client() -> LLMClient:
    """–ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä LLM –∫–ª–∏–µ–Ω—Ç–∞"""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client
